---
title: "Block2 project"
author: "Biel Caballero and Gerard Gomez"
date: "2023-10-16"
output: html_document
---

```{r}
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(e1071)
library(corrplot)
library(cluster)
library(GGally)
library(cowplot)
library(ggpubr)
library(e1071)
library(caret)
library(LiblineaR)
library(randomForest)
```

## First we read the file:
```{r}
#setwd("/Users/Gerard/Desktop/Master/S3/AML/Block2 lab/")
#setwd("/Users/bielcave/Documents/MDS/3rd_Semester/AML/Project/")
data <- read.csv(file = "heart_data.csv")
head(data)
```

## Preprocessing

First, we remove the first two columns as they do not add more information to the dataset:
```{r}
data<-data[,-c(1,2)]
head(data)
```

After that, we compute the age of each individual in years (as the data is in terms of days):
```{r}
data[,1]<-round((data[,1]/365),0)
head(data)
```


Next we correct the data type and set the categories as factors instead of integers:
```{r}
data$gender <- as.factor(data$gender)
data$cholesterol <- as.factor(data$cholesterol)
data$gluc <- as.factor(data$gluc)
data$smoke <- as.factor(data$smoke)
data$alco <- as.factor(data$alco)
data$active <- as.factor(data$active)
data$cardio <- as.factor(data$cardio)
str(data)                
```
## Data Cleaning

We have found some extremely high and extremely low (40 mm Hg from the normal range) values that do not fall in the range of the American Heart Association.
![Table with the ranges for systolic and diastolic blood pressure](blood_pressure.png)

```{r}
df <- data[data$ap_hi <= 200 & data$ap_hi >= 100 & data$ap_lo <= 140 & data$ap_lo >= 60 & data$height >= 120 & data$weight >= 40, ] 
```

## Separe data between train and test:
Then we separate the data into training and testing using 70% of the data for train, and 30% for test. 
```{r}
## 70% of the sample size
smp_size <- floor(0.7 * nrow(df))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

train <- df[train_ind, ]
test <- df[-train_ind, ]
```

## Explotation Data Analysis
First we start with the univariate analysis
```{r}
# Categorical data

gender<-ggplot(train, aes(x = gender, fill = gender)) + 
  geom_histogram(stat="count") + 
  labs(x = "gender") + 
  ggtitle("gender") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

cholesterol<-ggplot(train, aes(x = cholesterol, fill = cholesterol)) + 
  geom_histogram(stat="count") + 
  labs(x = "cholesterol") + 
  ggtitle("cholesterol") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

gluc<-ggplot(train, aes(x = gluc, fill = gluc)) + 
  geom_histogram(stat="count") + 
  labs(x = "gluc") + 
  ggtitle("gluc") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

smoke<-ggplot(train, aes(x = smoke, fill = smoke)) + 
  geom_histogram(stat="count") + 
  labs(x = "smoke") + 
  ggtitle("smoke") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

alco<-ggplot(train, aes(x = alco, fill=alco)) + 
  geom_histogram(stat="count") + 
  labs(x = "alco") + 
  ggtitle("alco") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

active<-ggplot(train, aes(x = active, fill=active)) + 
  geom_histogram(stat="count") + 
  labs(x = "active") + 
  ggtitle("active") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

cardio<-ggplot(train, aes(x = cardio, fill=cardio)) + 
  geom_histogram(stat="count") + 
  labs(x = "cardio") + 
  ggtitle("cardio") +
  theme_minimal() + 
  theme(plot.title = element_text(color="black", size=14, face="bold"),
        axis.line.x = element_line(),
        axis.line.y = element_line())

p<-plot_grid(gender, cholesterol, gluc, smoke, alco, active, cardio)
p
ggsave("univariate_histograms.pdf", p)
```

Now we continue with the univariate analysis of the numerical variables:
```{r}
boxplot(train[,c("age","height","weight","ap_hi","ap_lo")])

```

Next we are interested in testing for normality for all our numerical variables. This will be useful in order to know which algorithms to choose, as some assume normality of the data
```{r}
age<-ggqqplot(train$age, title = "age")
height<-ggqqplot(train$height, title = "height")
weight<-ggqqplot(train$weight, title = "weight")
ap_hi<-ggqqplot(train$ap_hi, title = "ap_hi")
ap_lo<-ggqqplot(train$ap_lo, title = "ap_lo")
ggarrange(age, height, weight, ap_hi, ap_lo)
```

## Bivariate analysis

```{r}
corr <- cor(train[,c("age","height","weight","ap_hi","ap_lo")],method = "spearman")

png("corrplot.png")
corr.plot <- corrplot(corr,type = "lower")
while (!is.null(dev.list()))  dev.off()
```

## PCA

```{r}
pca_df <- PCA(train[,c("age","height","weight","ap_hi","ap_lo")], graph = F,scale.unit = T)

# show scree plot of PCA
scree_plot <- fviz_screeplot(pca_df, addlabels = TRUE) # With the three fist principal we can explain an 81.6% of the data 
ggsave("screeplot.png", plot = scree_plot)
```

```{r}
pca.y = as.data.frame(pca_df$ind$coord)
ind.smpl.10000 = sample(1:nrow(pca.y),size = 10000,replace = F)

PCA_12 <- ggplot(pca.y[ind.smpl.10000,], aes(x = Dim.1 , y = Dim.2, col = train$cardio[ind.smpl.10000])) + 
  geom_point() + 
  theme_minimal() + 
  theme(axis.line = element_line()) +
  scale_color_manual("Cardio", values = c("#1F968BFF", "#FDE725FF"), labels = c("No","Yes"))

PCA_13 <- ggplot(pca.y[ind.smpl.10000,], aes(x = Dim.1 , y = Dim.3, col = train$cardio[ind.smpl.10000])) + 
  geom_point() + 
  theme_minimal() + 
  theme(axis.line = element_line()) +
  scale_color_manual("Cardio", values = c("#1F968BFF", "#FDE725FF"), labels = c("No","Yes"))

PCA_23 <- ggplot(pca.y[ind.smpl.10000,], aes(x = Dim.2 , y = Dim.3, col = train$cardio[ind.smpl.10000])) + 
  geom_point() + 
  theme_minimal() + 
  theme(axis.line = element_line()) +
  scale_color_manual("Cardio", values = c("#1F968BFF", "#FDE725FF"), labels = c("No","Yes"))

ggsave("pca_12.png", plot = PCA_12)
ggsave("pca_13.png", plot = PCA_13)
ggsave("pca_23.png", plot = PCA_23)
```

```{r}
varplot_12 <- fviz_pca_var(pca_df, axes = c(1,2), col.var = "steelblue")
varplot_13 <- fviz_pca_var(pca_df, axes = c(1,3), col.var = "steelblue")
varplot_23 <- fviz_pca_var(pca_df, axes = c(2,3), col.var = "steelblue")

ggsave("var_12.png", plot = varplot_12)
ggsave("var_13.png", plot = varplot_13)
ggsave("var_23.png", plot = varplot_23)
```
## Clustering 

```{r}
png("sil.png")
fviz_nbclust(train[ind.smpl.10000,c("age","height","weight","ap_hi","ap_lo")], kmeans, method = "silhouette")
while (!is.null(dev.list()))  dev.off()
```
We can see that the optimal k is at 2 clusters

```{r}
k.train <- kmeans(train[,c("age","height","weight","ap_hi","ap_lo")],centers = 2)

png("cluster_pairs.png")
ggpairs(train[,c("age","height","weight","ap_hi","ap_lo")],aes(col=as.factor(k.train$cluster)))
while (!is.null(dev.list()))  dev.off()
```


## Modeling

```{r}
f1_score <- function(model,data,kernel=TRUE){
  if (kernel){
    pred <- predict(model, newdata = data[-12])
    conf.mat <- table(pred,t(data[12]))
  }
  else {
    pred <- predict(model,newx = data[-12])
    conf.mat <- table(pred$prediction,t(data[12]))
    
  }
  
  
  precision.0 <- (conf.mat[1])/(conf.mat[1]+conf.mat[3])
  recall.0 <- (conf.mat[1])/(conf.mat[1]+conf.mat[2])
  f1_score.0 <- (2*precision.0*recall.0)/(precision.0+recall.0)
  
  precision.1 <- (conf.mat[4])/(conf.mat[1]+conf.mat[2])
  recall.1 <- (conf.mat[4])/(conf.mat[1]+conf.mat[3])
  f1_score.1 <- (2*precision.1*recall.1)/(precision.1+recall.1)
  
  return(list(f1.0 = f1_score.0, f1.1 = f1_score.1))
}
```

```{r}
## 70% of the sample size
smp_size <- floor(0.7 * nrow(train))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(train)), size = 10000)
train_model <- train[train_ind, ]
validation_ind <- sample(seq_len(nrow(train))[-train_ind], size = 5000)
validation <- train[validation_ind, ]

train_model[,c("age","height","weight","ap_hi","ap_lo")] = scale(train_model[,c("age","height","weight","ap_hi","ap_lo")])
#validation[,c("age","height","weight","ap_hi","ap_lo")] = scale(validation[,c("age","height","weight","ap_hi","ap_lo")])

```

### SVM kernelization:

```{r}
nu_list <- seq(0.1,0.9, by = 0.1)
accuracy_list <- numeric(length(nu_list))

for(i in 1:length(nu_list)){
  svm.kernel <- svm(cardio ~ ., data=train_model, kernel = "radial",type = "nu-classification",nu=nu_list[i],cross = 10)
  accuracy_list[i] <- svm.kernel$tot.accuracy
}

svm.kernel <- svm(cardio ~ ., data=train_model, kernel = "radial",type = "nu-classification",nu=nu_list[which.max(accuracy_list)])

f1_score(svm.kernel,validation)
```


### SVM (non-kernelized):
```{r}

tryTypes=c(0,2)
tryCosts=seq(0.1,0.9, by = 0.05)
bestCost=NA
bestAcc=0
bestType=NA

for(ty in tryTypes){
	for(co in tryCosts){
		acc=LiblineaR(data=train_model[-12],target=train_model[12],type=ty,cost=co,bias=1,cross=10,verbose=FALSE)
		cat("Results for C=",co," : ",acc," accuracy.\n",sep="")
		if(acc>bestAcc){
			bestCost=co
			bestAcc=acc
			bestType=ty
		}
	}
}

cat("Best model type is:",bestType,"\n")
cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n")

svm <- LiblineaR(data=train_model[-12],target=train_model[12],type=bestType,cost=bestCost,bias=1,verbose=FALSE)

f1_score(svm,validation,FALSE)
```

### Random Forest
```{r}
num_tree <- c(50,100,200,300,400,500)
accuracy_list = numeric(length(num_tree))
for(i in 1:length(num_tree)){
  rf = rfcv(train_model[,-12],train_model[,12],cv.fold = 10,ntree = num_tree[i],recursive = T)
  accuracy_list[i] <- rf$error.cv[which.min(rf$error.cv)]
}

rf <- randomForest(cardio~., data = train_model, ntree=num_tree[which.min(accuracy_list)])
f1_score(rf,validation)
```
## Final model

```{r}
bestmodel = randomForest(cardio~.,data = train,ntree = 300)

f1_score(bestmodel,test)
```

```{r}
varImpPlot(bestmodel)
```

